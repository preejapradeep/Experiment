{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e62bf1f",
   "metadata": {},
   "source": [
    "# Leave-One-In Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b638b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Install the edist library using pip\n",
    "# !pip install edist\n",
    "\n",
    "# Import the modules from edist\n",
    "import edist.ted as ted\n",
    "import edist.sed as sed\n",
    "import edist.aed as aed\n",
    "import edist.dtw as dtw\n",
    "import edist.seted as seted\n",
    "\n",
    "# Other imports\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c018d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertion_cost = 1. # Cost of insertion\n",
    "deletion_cost = 1. # Cost of deletion\n",
    "leave_change = 1. # Cost of substitution\n",
    "default_cost = 100 # Default cost \n",
    "\n",
    "start_edit = 1  # Define the starting edit value\n",
    "end_edit = 21   # Define the ending edit value\n",
    "\n",
    "num_cases = 300  # Define the number of cases to slice from the beginning of the list\n",
    "casebase = \"casebase300.json\" # Store the casebase filename\n",
    "directory = '' # Add the directory path\n",
    "\n",
    "structural_similarity = [\"Levenshtein Distance\", \"Affine Edit Distance\", \"Dynamic Time Warping\", \"Set Edit Distance\", \"Tree Edit Distance\"]\n",
    "explainerNames=[\"/Images/Anchors\",\"/Images/Counterfactuals\",\"/Images/GradCamTorch\",\"/Images/IG\", \"/Images/LIME\", \"/Tabular/ALE\", \"/Tabular/Anchors\",\"/Tabular/DeepSHAPGlobal\", \"/Tabular/DeepSHAPLocal\", \"/Tabular/DicePrivate\",\"/Tabular/DicePublic\",\"/Tabular/DisCERN\",\"/Text/NLPClassifier\",\"/Timeseries/CBRFox\",\"/Tabular/IREX\", \"/Tabular/Importance\", \"/Text/LIME\", \"/Tabular/LIME\", \"/Tabular/NICE\", \"/Tabular/TreeSHAPGlobal\", \"/Tabular/TreeSHAPLocal\", \"/Tabular/KernelSHAPGlobal\", \"/Tabular/KernelSHAPLocal\"]\n",
    "control_nodes = np.array([[\"s\", \"p\"]]) # Assume control_nodes is a 2-dimensional array\n",
    "\n",
    "used_nodes = set()  # used nodes\n",
    "unused_nodes = set() # unused nodes\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8039154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder to store the random cases\n",
    "folder_name = \"BT_Random\" \n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "random_bt_output = os.path.join(folder_name, \"RandomBT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f27b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta: custom node distance function\n",
    "def semantic_delta(x, y):\n",
    "    if(x==y):\n",
    "         ret = 0.\n",
    "    elif(x!=None and y==None): #insertion\n",
    "         ret = insertion_cost\n",
    "    elif(x==None and y!=None): #deletion \n",
    "        ret = deletion_cost\n",
    "    elif(x=='r'or y=='r'):  #we assign an infinite cost when comparing a root node\n",
    "        ret = np.inf\n",
    "    elif(x in ['s','p'] and y in['s','p']): #if both nodes are either sequence or priority, assign null cost\n",
    "        ret = 0.\n",
    "    elif(x in ['s','p'] or y in ['s','p']): #if one of the nodes is a sequence or priority, the other won't because of the previous rule\n",
    "        ret = np.inf\n",
    "    elif(x[0] == '/' and y[0]=='/'):\n",
    "        ret =  leave_change  # substitution of explainer\n",
    "    else:\n",
    "        ret = leave_change  # substitution of explainer\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3212874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of explainers in the same order specified by the tree\n",
    "def explainer_sequence(bt,node,adj,seq):\n",
    "    seq.append(node)\n",
    "    if adj: \n",
    "        for child in adj:\n",
    "            explainer_sequence(bt, bt[\"nodes\"][child],bt[\"adj\"][child],seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b980683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TED computation\n",
    "def ted_similarity(q,c,delta):\n",
    "    s1=[]\n",
    "    explainer_sequence(q,q[\"nodes\"][0],q[\"adj\"][0],s1)\n",
    "    s2=[]\n",
    "    explainer_sequence(c,c[\"nodes\"][0],c[\"adj\"][0],s2)\n",
    "\n",
    "    dist = ted.ted(q[\"nodes\"], q[\"adj\"], c[\"nodes\"], c[\"adj\"],delta)\n",
    "    \n",
    "    return  dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81dd1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence edit distance of Levenshtein (1965)\n",
    "def levenshtein_similarity(q,c, delta=None):\n",
    "    s1=[]\n",
    "    explainer_sequence(q,q[\"nodes\"][0],q[\"adj\"][0],s1)\n",
    "    s2=[]\n",
    "    explainer_sequence(c,c[\"nodes\"][0],c[\"adj\"][0],s2)\n",
    "\n",
    "    dist = sed.sed(s1,s2, delta)\n",
    "    \n",
    "    return  dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6be584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence edit distance with affine gap costs using algebraic dynamic programming (ADP; Giegerich, Meyer, and Steffen, 2004),\n",
    "# as applied by Paa√üen, Mokbel, and Hammer (2016)\n",
    "def aed_similarity(q,c, delta=None):\n",
    "    s1=[]\n",
    "    explainer_sequence(q,q[\"nodes\"][0],q[\"adj\"][0],s1)\n",
    "    s2=[]\n",
    "    explainer_sequence(c,c[\"nodes\"][0],c[\"adj\"][0],s2)\n",
    "    dist = aed.aed(s1,s2,delta)\n",
    "    \n",
    "    return  dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd9875ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_dtw_distance(x,y):\n",
    "    if x==y:\n",
    "        return 0\n",
    "    else: \n",
    "        return insertion_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18a06490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic time warping distance of Vintsyuk (1968)    \n",
    "def dtw_similarity(q,c, delta=None):\n",
    "    s1=[]\n",
    "    explainer_sequence(q,q[\"nodes\"][0],q[\"adj\"][0],s1)\n",
    "    s2=[]\n",
    "    explainer_sequence(c,c[\"nodes\"][0],c[\"adj\"][0],s2)\n",
    "    if delta==None:\n",
    "        delta = default_dtw_distance\n",
    "    dist = dtw.dtw(s1,s2,delta)\n",
    "    \n",
    "    return  dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44b65f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hungarian algorithm of Kuhn, 1955 \n",
    "def set_similarity(q,c, delta=None):\n",
    "    s1=[]\n",
    "    explainer_sequence(q,q[\"nodes\"][0],q[\"adj\"][0],s1)\n",
    "    s2=[]\n",
    "    explainer_sequence(c,c[\"nodes\"][0],c[\"adj\"][0],s2)\n",
    "    dist = seted.seted(s1,s2,delta)\n",
    "    \n",
    "    return  dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b66c62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the node is a control_node\n",
    "def is_control_node(node, node_index):\n",
    "    # print('node:',node,'edge:',edge,'\\n')\n",
    "    if len(node) < 2 and node_index != 0:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8cc3328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a node for substitution or replacement\n",
    "def get_replacement_node(nodes, control_nodes):\n",
    "    for i, control_set in enumerate(control_nodes):\n",
    "        if control_set[0] in nodes:\n",
    "            if len(control_set) == 1:\n",
    "                continue\n",
    "            if len(control_set) > 1 and len(control_set) >= 2 and control_set[1] not in nodes:\n",
    "                return control_set[1]\n",
    "        elif control_set[1] in nodes:\n",
    "            if control_set[0] not in nodes:\n",
    "                return control_set[0]\n",
    "        else:\n",
    "            return control_set[0]\n",
    "    return control_nodes[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30f62dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new node for edit operation\n",
    "def get_new_node(nodes, used_nodes, operation):\n",
    "    discard_nodes = ['r', 'f', 't']\n",
    "    unused_nodes = [node for node in (set(nodes) - set(used_nodes)) if node not in discard_nodes]\n",
    "    exp_nodes = list(set(explainerNames)) # in case of repeated explainers\n",
    "    # exp_nodes = list(set(explainerNames) - set(nodes)) # in case of nonrepeated explainers\n",
    "    # cont_nodes = list(set(control_nodes.flatten()) - set(nodes))\n",
    "\n",
    "    if not unused_nodes:\n",
    "        operation = 'insertion'\n",
    "        new_node = random.choice(exp_nodes)\n",
    "        node_index = None\n",
    "        node = None  \n",
    "        return operation, node_index, node, new_node\n",
    "\n",
    "    if operation == 'replacement':\n",
    "        unusednodes = [node for node in unused_nodes if node not in control_nodes]\n",
    "        if not unusednodes:\n",
    "            operation = 'insertion'\n",
    "            new_node = random.choice(exp_nodes)\n",
    "            node_index = None\n",
    "            node = None\n",
    "            return operation, node_index, node, new_node\n",
    "        else:\n",
    "            node = np.random.choice(unusednodes)\n",
    "            node_index = nodes.index(node)\n",
    "            new_node = ''\n",
    "\n",
    "            if node[0] == '/':\n",
    "                # node is an explainer\n",
    "                if len(exp_nodes) > 0:\n",
    "                    new_node = np.random.choice(exp_nodes)\n",
    "                else:\n",
    "                    new_node = None\n",
    "            # else:\n",
    "            #     # node is a control node\n",
    "            #     unused_control_nodes = set(control_nodes.flatten()) - used_nodes\n",
    "            #     print('\\nunused_control_nodes:', unused_control_nodes)\n",
    "            #     if len(unused_control_nodes) > 0:\n",
    "            #         new_node = np.random.choice(control_nodes.flatten())\n",
    "            #         print(\"c - new_node\",new_node)\n",
    "            #     else:\n",
    "            #         print(\"New node\")\n",
    "\n",
    "    elif operation == 'deletion':\n",
    "        discard_nodes = ['r', 'f', 't']\n",
    "        unused_control_nodes = [node for node in nodes if not node.startswith('/') and node not in used_nodes and node not in discard_nodes]\n",
    "        combined_nodes = unused_nodes + unused_control_nodes\n",
    "        node = np.random.choice(combined_nodes)            \n",
    "        node_index = nodes.index(node)\n",
    "        new_node = ''\n",
    "\n",
    "        if not unused_nodes:\n",
    "            operation = 'insertion'\n",
    "            operation, node_index, node, new_node = get_new_node(nodes, used_nodes, operation) \n",
    "        elif not unused_control_nodes:\n",
    "            operation = np.random.choice(['replacement','insertion'])\n",
    "            operation, node_index, node, new_node = get_new_node(nodes, used_nodes, operation)\n",
    "        elif len(unused_control_nodes) == 1:\n",
    "            operation = 'insertion'\n",
    "            operation, node_index, node, new_node = get_new_node(nodes, used_nodes, operation)   \n",
    "        else: \n",
    "            operation = 'deletion'  \n",
    "\n",
    "    elif operation == 'insertion':\n",
    "        new_node = random.choice(exp_nodes)\n",
    "        node_index = None\n",
    "        node = None  \n",
    "\n",
    "    return operation, node_index, node, new_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a420b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert edge list to adjacency list\n",
    "def ELtoAL(edges,nodes):       #converting edge list to adjacency list\n",
    "    node_index,adj_dict = {},{}\n",
    "    adj_list=[]\n",
    "    for index, value in enumerate(nodes):\n",
    "        node_index[value]=index\n",
    "    for edge in edges:  \n",
    "        u,v = edge\n",
    "        u=node_index[edge[0]]\n",
    "        v=node_index[edge[1]]\n",
    "        if u not in adj_dict:\n",
    "            adj_dict[u] = []\n",
    "        if v not in adj_dict:\n",
    "            adj_dict[v] = []\n",
    "        adj_dict[u].append(v)\n",
    "\n",
    "    for adj in list(adj_dict):\n",
    "        adj_list.append(adj_dict[adj])\n",
    "\n",
    "    return adj_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "079e3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert adjacency list to edge list\n",
    "def ALtoEL(nodes,adj): #converting adjacency list to edge list\n",
    "    edgelist =[]\n",
    "    node_ind,adj_index={},{}\n",
    "    for index, value in enumerate(nodes):      \n",
    "        for ind, val in enumerate(adj):   \n",
    "            node_ind[index]=value\n",
    "            adj_index[ind]=val\n",
    "\n",
    "    for i in adj_index:\n",
    "        for ad in adj_index[i]:\n",
    "            if ad is not None: \n",
    "                u=node_ind[i]\n",
    "                v=node_ind[ad]\n",
    "                edge=(u,v)\n",
    "                edgelist.append(edge)\n",
    "            else:\n",
    "                continue\n",
    "    return edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be56ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the random edit operation for random bt\n",
    "def choose_random_operation(random_bt_prime, edits):\n",
    "    nodes = random_bt_prime['nodes']\n",
    "    adj = random_bt_prime['adj']\n",
    "    edge = ALtoEL(nodes,adj)\n",
    "    num_nodes = len(nodes)\n",
    "    global num_operations\n",
    "    num_operations = 0\n",
    "\n",
    "    # initialize the children list for each node\n",
    "    node_list = [{'id': node, 'children': []} for node in nodes]\n",
    "    operation = ''\n",
    "    \n",
    "    if num_nodes >= 2:\n",
    "        if num_nodes <= 3:\n",
    "            operation = 'insertion'\n",
    "        else:\n",
    "            operation = np.random.choice(['replacement', 'insertion', 'deletion'])\n",
    "        operation, node_index, node, new_node = get_new_node(nodes, used_nodes, operation)\n",
    "        \n",
    "        # if new_node not in nodes and new_node not in used_nodes and node not in used_nodes:\n",
    "        if operation == 'replacement':\n",
    "            # Perform the replacement\n",
    "            if node[0] == '/':\n",
    "                nodes[node_index] = new_node\n",
    "            else:\n",
    "                if node in control_nodes:\n",
    "                    replacement_node = get_replacement_node(nodes, control_nodes)\n",
    "                    nodes[nodes.index(node)] = replacement_node\n",
    "                    edge = [(e[0] if e[0] != node else replacement_node, e[1] if e[1] != node else replacement_node) for e in edge]\n",
    "                    random_bt_prime['edge'] = edge\n",
    "        \n",
    "        elif operation == 'deletion':  # Perform the deletion\n",
    "            node = nodes[node_index]\n",
    "            # node is not the root\n",
    "            if node_index != 1:\n",
    "                # Check whether it is a control node or explainer\n",
    "                if is_control_node(node, edge):\n",
    "                    print('node', node, adj)\n",
    "                    parent_index = None\n",
    "                    for i, sublist in enumerate(adj):\n",
    "                        if node_index in sublist:\n",
    "                            parent_node = nodes[i]\n",
    "                            parent_index = i\n",
    "                    # To delete a control node, move its children to the parent node of that control node \n",
    "                    if parent_index is None:\n",
    "                        choose_random_operation(random_bt_prime, edits)\n",
    "                    else:\n",
    "                        # Append the children of the node to the parent node's adjacency list\n",
    "                        adj[parent_index].extend(adj[node_index])\n",
    "                        # Remove the adjacency list at node_index\n",
    "                        del adj[node_index]\n",
    "                        # Update the adjacency list to remove references to the deleted node and adjust indices\n",
    "                        adj = [[idx-1 if idx > node_index else idx for idx in sublist if idx != node_index] for sublist in adj]\n",
    "                        # Remove the node from the lists \n",
    "                        nodes.pop(node_index)\n",
    "                        random_bt_prime['adj'] = adj\n",
    "                    \n",
    "                elif node[0] == '/':\n",
    "                    # delete the node from the nodes list\n",
    "                    del nodes[node_index]\n",
    "                    # Remove the adjacency list at node_index\n",
    "                    del adj[node_index]\n",
    "                    adj = [[idx for idx in sublist if idx != node_index] for sublist in adj]\n",
    "                    for i, neighbors in enumerate(adj):\n",
    "                        updated_neighbors = [n - 1 if n > node_index else n for n in neighbors]\n",
    "                        adj[i] = updated_neighbors\n",
    "                    random_bt_prime['adj'] = adj\n",
    "                else:\n",
    "                    choose_random_operation(random_bt_prime, edits)\n",
    "            else:\n",
    "                print(\"Can't delete root node. Choose another node.\")\n",
    "                choose_random_operation(random_bt_prime, edits)\n",
    "\n",
    "        elif operation == 'insertion':\n",
    "            # new node is an explainer, insert as leaf node\n",
    "            if new_node[0] == '/':\n",
    "                discard_nodes = ['r', 'f', 't']\n",
    "                c_nodes = list(filter(lambda node: not node.startswith('/') and node not in discard_nodes, nodes))\n",
    "                parent_node = np.random.choice(list(set(c_nodes)))\n",
    "                if parent_node in nodes:\n",
    "                    parent_index = nodes.index(parent_node)\n",
    "                    parent_adjacency_list = adj[parent_index]\n",
    "                    if len(parent_adjacency_list) > 0:\n",
    "                        first_node_index = parent_adjacency_list[0]\n",
    "                        node_index_new = first_node_index\n",
    "                        nodes.insert(node_index_new, new_node)\n",
    "                        for i, node_adj in enumerate(adj):\n",
    "                            adj[i] = [idx + 1 if idx > node_index_new - 1 else idx for idx in node_adj]\n",
    "                        # Add the new_node index in front of the first node in adj[parent_index]\n",
    "                        adj[parent_index].insert(0, node_index_new)\n",
    "                        # Insert an empty list at the node_index_new position in the adjacency list\n",
    "                        adj.insert(node_index_new, [])\n",
    "                        # break\n",
    "                    else:\n",
    "                        print(\"The parent_adjacency_list is empty.\")\n",
    "                        choose_random_operation(random_bt_prime, edits)\n",
    "                else:\n",
    "                    node_index_new = len(nodes)\n",
    "                    nodes.append(new_node)\n",
    "                    # append new empty adjacency list to adj\n",
    "                    adj.append([])\n",
    "                    # update parent node's adjacency list\n",
    "                    adj[parent_index].append(node_index_new)\n",
    "                    adj[node_index_new].append(parent_index)\n",
    "                    edge.append((parent_node, new_node))\n",
    "                    adj = ELtoAL(edge,nodes)\n",
    "                    random_bt_prime['adj'] = adj\n",
    "            else:\n",
    "                # new node is a control node, insert as root or parent node\n",
    "                nodes.insert(0, new_node)\n",
    "                adj.insert(0, [])\n",
    "                for i in range(num_nodes-1):\n",
    "                    for j, val in enumerate(adj[i]):\n",
    "                        if val >= node_index:\n",
    "                            adj[i][j] += 1\n",
    "                edge.append((node, new_node))\n",
    "    else:\n",
    "        return random_bt_prime, edits, operation      \n",
    "        \n",
    "    print('Final - edits:',edits, 'random_bt_prime:', random_bt_prime)   \n",
    "    print('*********************************************************\\n')   \n",
    "\n",
    "    end_time = time.time()\n",
    "    computation_time = end_time - start_time\n",
    "    print(\"\\nComputation time:\", computation_time, \"seconds\", ) \n",
    "\n",
    "    return random_bt_prime, edits, operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da5c0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the edit distance for the five structural metrics\n",
    "def compute_edit_distance(random_index, random_bt, random_bt_dict, case_base, case_base_dict, structural_similarity):    \n",
    "    # global explainer_dataframe\n",
    "    random_bt = case_base[random_index]\n",
    "    random_bt_dict[random_index]=random_bt\n",
    "    random_bt_p = copy.deepcopy(random_bt)\n",
    "    results = pd.DataFrame(columns=['Random BT', 'Edits', 'Operation', 'Modified BT', 'Case', 'Structural Similarity', 'Edit Distance'])\n",
    "    \n",
    "    # Loop to perform operations on the BT - maximum operation is the length of BT\n",
    "    for edits in range(start_edit, end_edit):\n",
    "        # Choose a random edit operation and get the new BT\n",
    "        random_bt_prime, edits, operation = choose_random_operation(random_bt_p, edits)\n",
    "        # For each structural_similarity, compute the edit distance using each similarity_metrics\n",
    "        for algorithm in structural_similarity:\n",
    "                # Compute the edit distance between the modified BT and all the BTs in the case base\n",
    "                for bt_name, bt in case_base_dict.items():\n",
    "                    # explainer_dataframe = pd.read_csv(metric,index_col=0)\n",
    "                    if algorithm == \"Tree Edit Distance\":\n",
    "                        score = ted_similarity(bt,random_bt_prime, delta=semantic_delta)\n",
    "                    elif algorithm == \"Levenshtein Distance\":\n",
    "                        score = levenshtein_similarity(bt,random_bt_prime, delta=semantic_delta)\n",
    "                    elif algorithm == \"Affine Edit Distance\":\n",
    "                        score = aed_similarity(bt,random_bt_prime, delta=semantic_delta)\n",
    "                    elif algorithm == \"Dynamic Time Warping\":\n",
    "                        score = dtw_similarity(bt,random_bt_prime, delta=semantic_delta)\n",
    "                    elif algorithm == \"Set Edit Distance\":\n",
    "                        score = set_similarity(bt,random_bt_prime, delta=semantic_delta)\n",
    "                    \n",
    "                    results.loc[len(results.index)] = [random_bt, edits, operation, random_bt_prime, bt, algorithm, score]\n",
    "                    \n",
    "        # Assign random_bt_prime to random_bt_p\n",
    "        random_bt_p = copy.deepcopy(random_bt_prime)\n",
    "\n",
    "    return results, random_bt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1efcee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load case base from json file\n",
    "with open(casebase, \"r\") as f:\n",
    "    case_base = json.load(f)\n",
    "\n",
    "case_base_dict, random_bt_dict = {},{}\n",
    "for i, bt in enumerate(case_base):\n",
    "    case_base_dict[f'bt_{i}'] = bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a3111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the CSV file in append mode\n",
    "with open(random_bt_output, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Random BT', 'Modified BT', 'Edits'])\n",
    "    results = pd.DataFrame(columns=['Random BT', 'Edits', 'Operation', 'Modified BT', 'Case', 'Structural Similarity', 'Edit Distance'])\n",
    "    ranking_results = pd.DataFrame(columns=['Random BT', 'Edits', 'Operation', 'Modified BT', 'Case', 'Structural Similarity', 'Edit Distance', 'Rank'])\n",
    "\n",
    "    # Loop over the random BTs\n",
    "    random_indices = []\n",
    "\n",
    "    # create a sublist of the random BTs\n",
    "    sublist = case_base[:num_cases]\n",
    "\n",
    "    # create args_list with the random BTs in the sublist\n",
    "    args_list = [(i, bt, random_bt_dict, case_base, case_base_dict, structural_similarity) for i, bt in enumerate(sublist)]\n",
    "    \n",
    "    # Create a pool of worker processes to run the function in parallel\n",
    "    with Pool() as pool:\n",
    "        print('Loading pool....')\n",
    "    # Map the function over the list of arguments and get the results\n",
    "        all_results = pool.starmap(compute_edit_distance, args_list)\n",
    "\n",
    "    for i in range(len(all_results)):\n",
    "        result = all_results[i][0]  # get the result at index i\n",
    "        # convert the result list to a pandas dataframe and add it to the new_results dataframe\n",
    "        random_bt_results = pd.DataFrame(result, columns=['Random BT', 'Edits', 'Operation', 'Modified BT', 'Case', 'Structural Similarity','Edit Distance'])\n",
    "        results = pd.concat([results, random_bt_results])        \n",
    "    file.close()\n",
    "\n",
    "    # create a dictionary to store the data frames for each sheet\n",
    "    sheet_dict = {}\n",
    "\n",
    "    for i in range(len(all_results)):\n",
    "        random_bt_dict = all_results[i][1]  # get the result_dict at index i\n",
    "\n",
    "        for key, bt in random_bt_dict.items():\n",
    "        # Rank the casebase based on edit distance for each metric\n",
    "            # filter results by the current random_bt\n",
    "            random_bt_results = results.loc[results['Random BT'] == random_bt_dict[key]]\n",
    "            # # Group the results by edits and metric\n",
    "            groups = random_bt_results.groupby(['Edits','Structural Similarity'])\n",
    "            ranks = groups['Edit Distance'].rank(method='dense')\n",
    "            # ranks = groups['Edit Distance'].rank(method='dense', ascending=False)\n",
    "            # # Add the ranks to the results DataFrame\n",
    "            random_bt_results['Rank'] = ranks\n",
    "            # Add the filtered results to the sheet dictionary\n",
    "            sheet_dict[key] = random_bt_results\n",
    "            # save filtered results to a csv file\n",
    "            random_bt_results.to_csv(directory + f'random_bt_{key}_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be208a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "computation_time = end_time - start_time\n",
    "print(\"\\nFinal Computation time:\", computation_time, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
